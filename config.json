{
    "return_attention_scores" : true,
    "hidden_size" : 128,
    "input_seq_len" : 40,
    "target_seq_len" : 40,
    "hidden_alignment_size" : 32,
    "attention" : "dot",
    "num_layers" : 3,
    "input_size" : 64, 
    "padding_idx" : 0,
    "encoder_vocab_size" : 5751,
    "decoder_vocab_size" : 3787,
    "hidden_alignement_size" : 32,
    "encoder_dropout" : 0.1,
    "decoder_dropout" : 0.1,
    "bidirectional" : true,
    "encoder" : "lstm",
    "decoder" : "lstm",
    "max_len" : 15,
    "batch_size" : 32,
    "train_val_split_factor" : 0.8,
    "learning_rate" : 1e-5,
    "max_epochs" : 10,
    "patience" : 3,
    "val_check_interval" : 0.5,
    "num_workers" : 4
}